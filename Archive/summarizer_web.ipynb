{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrvZj0WWEQvx"
   },
   "outputs": [],
   "source": [
    "!lsof -i :5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-Ch6PfREXiN"
   },
   "outputs": [],
   "source": [
    "!kill -9 893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSdjmqV_EZq5",
    "outputId": "49fd9078-c186-4fee-c3ec-dcc2e86f1b2f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIEBAnrUEcKL",
    "outputId": "7c614c37-9c9f-4b7a-b24d-3cb6d7749742"
   },
   "outputs": [],
   "source": [
    "!apt-get install nodejs npm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glP1nFvVEejd",
    "outputId": "cf348511-9f50-452e-9249-1e12c86ee0e9"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project\n",
    "\n",
    "!npm install\n",
    "!npm run build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "_wtsgvukFnyp",
    "outputId": "62a45bf4-cdce-4752-be06-92fc7041dffc"
   },
   "outputs": [],
   "source": [
    "nvm --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLAlXxjDEjzo",
    "outputId": "a17f85d9-f356-4848-f788-446e8e694f85"
   },
   "outputs": [],
   "source": [
    "!node -v\n",
    "!apt remove nodejs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZnEKp6iElfw",
    "outputId": "b4e92686-178b-4654-e60f-a9122b83a48c"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://deb.nodesource.com/setup_18.x | bash -\n",
    "!apt install -y nodejs\n",
    "!node -v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZb3W7ePEnZj",
    "outputId": "67579c1b-5ad1-438f-d0d7-da903c3d57db"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project\n",
    "!npm install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srWUre5MEqvF"
   },
   "outputs": [],
   "source": [
    "\n",
    "!chmod +x node_modules/@esbuild/linux-x64/bin/esbuild\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mQZ0uE5EsYr",
    "outputId": "97395191-da17-45c8-eee7-9b76e8871222"
   },
   "outputs": [],
   "source": [
    "!npm run build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nmeqW4MEvLO"
   },
   "outputs": [],
   "source": [
    "!ls -l /content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project/build\n",
    "!ls -l /content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project/dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVYhd4HbExVw",
    "outputId": "d36fdf2f-7c04-4249-8f91-7a2751c77872"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project\n",
    "!npm run dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtAIMpVHEy6Z",
    "outputId": "66e656a8-605e-4151-e1e5-f9b8493ce4df"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install flask flask-ngrok transformers torch sentence-transformers pdfplumber python-docx spacy yake flask-cors\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import os\n",
    "import pdfplumber\n",
    "import docx\n",
    "import spacy\n",
    "import yake\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Summarization Models\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for frontend-backend interaction\n",
    "run_with_ngrok(app)  # Enable ngrok\n",
    "\n",
    "FRONTEND_DIR = \"/content/drive/MyDrive/project-bolt-sb1-x4xjyrpa/project\"\n",
    "\n",
    "\n",
    "# Serve static files from the frontend directory\n",
    "@app.route(\"/<path:filename>\", methods=[\"GET\"])\n",
    "def serve_frontend(filename):\n",
    "    return send_from_directory(FRONTEND_DIR, filename)\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def index():\n",
    "    return send_from_directory(FRONTEND_DIR, \"index.html\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text += extracted_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(docx_file):\n",
    "    doc = docx.Document(docx_file)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Extractive summary function\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences[:num_sentences]\n",
    "\n",
    "# Keyword extraction function\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    kw_extractor = yake.KeywordExtractor(top=num_keywords, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "# Document classification function\n",
    "def classify_document(text):\n",
    "    categories = {\n",
    "        \"legal\": [\"court\", \"law\", \"agreement\", \"contract\", \"policy\"],\n",
    "        \"technical\": [\"AI\", \"algorithm\", \"data\", \"software\", \"engineering\"],\n",
    "        \"academic\": [\"research\", \"study\", \"university\", \"experiment\", \"paper\"],\n",
    "        \"general\": [\"news\", \"blog\", \"report\", \"story\", \"review\"]\n",
    "    }\n",
    "    doc = nlp(text.lower())\n",
    "    word_counts = {category: sum(1 for token in doc if token.text in words) for category, words in categories.items()}\n",
    "    return max(word_counts, key=word_counts.get).capitalize()\n",
    "\n",
    "# Function to generate abstractive summary\n",
    "def abstractive_summary(text, model, tokenizer, max_length=150):\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Function for single-sentence explanation using Bart\n",
    "def single_sentence_explanation(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = bart_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    output_ids = bart_model.generate(input_ids, max_length=30, num_beams=5, early_stopping=True)\n",
    "    return bart_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate summary function\n",
    "def generate_summary(text, num_extractive=5):\n",
    "    key_points = extractive_summary(text, num_extractive)\n",
    "    extracted_text = \" \".join(key_points)\n",
    "\n",
    "    t5_summary = abstractive_summary(extracted_text, t5_model, t5_tokenizer)\n",
    "    final_summary = t5_summary\n",
    "    explanation = single_sentence_explanation(extracted_text)\n",
    "    keywords = extract_keywords(text, num_keywords=5)\n",
    "    document_type = classify_document(text)\n",
    "\n",
    "    return key_points, final_summary, explanation, keywords, document_type\n",
    "\n",
    "# API endpoint for document processing\n",
    "@app.route(\"/process\", methods=[\"POST\"])\n",
    "def process_document():\n",
    "    uploaded_file = request.files.get(\"file\")\n",
    "    input_text = request.form.get(\"text\")\n",
    "\n",
    "    if uploaded_file:\n",
    "        file_ext = uploaded_file.filename.split(\".\")[-1]\n",
    "        if file_ext == \"pdf\":\n",
    "            extracted_text = extract_text_from_pdf(uploaded_file)\n",
    "        elif file_ext in [\"docx\", \"doc\"]:\n",
    "            extracted_text = extract_text_from_docx(uploaded_file)\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Unsupported file type\"}), 400\n",
    "    elif input_text:\n",
    "        extracted_text = input_text\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No input provided\"}), 400\n",
    "\n",
    "    key_points, summary, explanation, keywords, document_type = generate_summary(extracted_text)\n",
    "\n",
    "    return jsonify({\n",
    "        \"document_type\": document_type,\n",
    "        \"keywords\": keywords,\n",
    "        \"key_points\": key_points,\n",
    "        \"summary\": summary,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "# API Health Check\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health_check():\n",
    "    return jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
