{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4BtOCPobFM8",
    "outputId": "4ee934e2-b2b5-4970-85d6-4dde2aff5f17"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch sentence-transformers pdfplumber python-docx spacy yake\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsPMBzMibblJ"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import spacy\n",
    "import yake\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration, BartTokenizer, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from google.colab import files\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHCXUdLFbd1o"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text += extracted_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Extract text from a DOCX file.\"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def get_text_from_file():\n",
    "    \"\"\"Upload and extract text from a file.\"\"\"\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        ext = filename.split(\".\")[-1]\n",
    "        if ext == \"pdf\":\n",
    "            return extract_text_from_pdf(filename)\n",
    "        elif ext == \"docx\":\n",
    "            return extract_text_from_docx(filename)\n",
    "        elif ext == \"txt\":\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "        else:\n",
    "            return \"Unsupported file format!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7U9nMcpbiGG"
   },
   "outputs": [],
   "source": [
    "def extractive_summary(text, num_sentences=5):\n",
    "    \"\"\"Extract key sentences using spaCy for sentence tokenization.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    # Select the first `num_sentences` as key points\n",
    "    return sentences[:num_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yYvGLNebjc1"
   },
   "outputs": [],
   "source": [
    "def extract_keywords(text, num_keywords=5):\n",
    "    \"\"\"Extract important keywords using YAKE!\"\"\"\n",
    "    kw_extractor = yake.KeywordExtractor(top=num_keywords, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IUSSH9lck2h"
   },
   "outputs": [],
   "source": [
    "def classify_document(text):\n",
    "    \"\"\"Classify document type (Legal, Technical, Academic, General).\"\"\"\n",
    "    categories = {\n",
    "        \"legal\": [\"court\", \"law\", \"agreement\", \"contract\", \"policy\"],\n",
    "        \"technical\": [\"AI\", \"algorithm\", \"data\", \"software\", \"engineering\"],\n",
    "        \"academic\": [\"research\", \"study\", \"university\", \"experiment\", \"paper\"],\n",
    "        \"general\": [\"news\", \"blog\", \"report\", \"story\", \"review\"]\n",
    "    }\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    word_counts = {category: sum(1 for token in doc if token.text in words) for category, words in categories.items()}\n",
    "    predicted_category = max(word_counts, key=word_counts.get)\n",
    "\n",
    "    return predicted_category.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcYrYEU4bld7",
    "outputId": "7d92a584-b71d-4c90-b01e-21197e9fbc75"
   },
   "outputs": [],
   "source": [
    "# Load Tokenizers and Models\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFz3alXYbm46"
   },
   "outputs": [],
   "source": [
    "def abstractive_summary(text, model, tokenizer, max_length=150):\n",
    "    \"\"\"Generate an abstractive summary using a Transformer model.\"\"\"\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5xkFqrn6Xk1"
   },
   "outputs": [],
   "source": [
    "def single_sentence_explanation(text):\n",
    "    \"\"\"Generate a one-sentence explanation using BART.\"\"\"\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = bart_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    output_ids = bart_model.generate(input_ids, max_length=30, num_beams=5, early_stopping=True)\n",
    "    explanation = bart_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prMd-YM1EofX"
   },
   "outputs": [],
   "source": [
    "def generate_summary(text, num_extractive=5):\n",
    "    \"\"\"Generate key points, a paragraph summary, one-sentence explanation, and keywords.\"\"\"\n",
    "    # Step 1: Extract key points\n",
    "    key_points = extractive_summary(text, num_extractive)\n",
    "    extracted_text = \" \".join(key_points)\n",
    "\n",
    "    # Step 2: Generate paragraph summary (T5 + PEGASUS)\n",
    "    t5_summary = abstractive_summary(extracted_text, t5_model, t5_tokenizer)\n",
    "    pegasus_summary = abstractive_summary(extracted_text, pegasus_model, pegasus_tokenizer)\n",
    "\n",
    "    # Merge T5 and PEGASUS summaries\n",
    "    final_paragraph_summary = t5_summary + \" \" + pegasus_summary\n",
    "\n",
    "    # Step 3: Generate single-sentence explanation using BART\n",
    "    explanation = single_sentence_explanation(extracted_text)\n",
    "\n",
    "    # Step 4: Extract keywords\n",
    "    keywords = extract_keywords(text, num_keywords=5)\n",
    "\n",
    "    # Step 5: Classify document type\n",
    "    document_type = classify_document(text)\n",
    "\n",
    "    return key_points, final_paragraph_summary, explanation, keywords, document_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "FGU39e94Ep2l",
    "outputId": "4d828888-1a6f-4cb5-f1e9-5dd644ee2ff0"
   },
   "outputs": [],
   "source": [
    "choice = input(\"Choose input method: \\n1. Enter text manually\\n2. Upload a document (PDF, DOCX, TXT)\\nEnter 1 or 2: \")\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "if choice == \"1\":\n",
    "    text = input(\"Enter the text to summarize:\\n\")\n",
    "\n",
    "elif choice == \"2\":\n",
    "    text = get_text_from_file()\n",
    "\n",
    "if text:\n",
    "    key_points, paragraph_summary, explanation, keywords, document_type = generate_summary(text)\n",
    "\n",
    "    print(\"\\nDocument Type: \", document_type)\n",
    "    print(\"\\nKeywords: \", \", \".join(keywords))\n",
    "    print(\"\\nKeypoints of this document:\\n\")\n",
    "    for idx, sentence in enumerate(key_points, 1):\n",
    "        print(f\"{idx}. {sentence}\")\n",
    "\n",
    "    print(\"\\nSummary of what is written in the document:\\n\")\n",
    "    print(paragraph_summary)\n",
    "\n",
    "    print(\"\\nExplanation in a single sentence:\\n\")\n",
    "    print(explanation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
